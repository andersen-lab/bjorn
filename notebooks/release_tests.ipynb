{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from Bio import Seq, SeqIO, AlignIO, Phylo, Align\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sheet = pd.read_csv('210406_SARS-CoV-2_samplesheet.csv', skiprows=19)\n",
    "sample_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sheet['Sample_ID'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_samples(sample_sheet: pd.DataFrame, \n",
    "                     private_ids: list=['LASV'], \n",
    "                     primer_ids: list=['iTru']) -> dict:\n",
    "    \"\"\"RELEASE functionality: Separate A-lab Sample Sheet into constituent sub-sheets for each primer set.\"\"\"\n",
    "    results = {}\n",
    "    samples_to_exclude = []\n",
    "    # separate out private sample IDs\n",
    "    results['_private_sheet'] = pd.concat([sample_sheet.loc[sample_sheet['Sample_ID'].str.contains(idx)] for idx in private_ids])\n",
    "    for sample_idx in results['_private_sheet']['Sample_ID'].unique():\n",
    "            samples_to_exclude.append(sample_idx)\n",
    "    for primer_idx in primer_ids:\n",
    "        results[f'{primer_idx}_primers'] = sample_sheet.loc[sample_sheet['I7_Index_ID'].str.contains(primer_idx)]\n",
    "        for sample_idx in results[f'{primer_idx}_primers']['Sample_ID'].unique():\n",
    "            samples_to_exclude.append(sample_idx)\n",
    "    results['OG_primers'] = sample_sheet.loc[~sample_sheet['Sample_ID'].isin(samples_to_exclude)]\n",
    "    return results\n",
    "res = separate_samples(sample_sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['_private_sheet'].to_csv('_SampleSheet.csv', index=False)\n",
    "\n",
    "res['iTru_primers'].to_csv('itru_primer_SampleSheet.csv', index=False)\n",
    "\n",
    "res['OG_primers'].to_csv('old_primer_SampleSheet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lins = pd.read_csv('/Users/al/Documents/scripps/analysis/alab_analysis/2021.03.29/2021-03-29_analysis_report_v2/old_primers_batch/msa/lineage_report_2021-03-30.csv')\n",
    "lins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_sample_ids = lins.loc[lins['lineage']=='P.1', 'taxon'].apply(lambda x: x.split('_')[1]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gisaid_meta = pd.read_csv('/Users/al/Documents/scripps/analysis/alab_release/2021-03-31_release/valhalla/2021-03-31_release/gisaid_metadata.csv')\n",
    "gisaid_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gisaid_meta['sample_id'] = gisaid_meta['Virus name'].apply(lambda x: x.split('/')[2])\n",
    "gisaid_meta['sample_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gisaid_meta.loc[gisaid_meta['sample_id'].isin(p1_sample_ids), 'Additional location information'] = 'travel-related'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gisaid_meta.drop(columns=['sample_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gisaid_meta.to_csv('/Users/al/Documents/scripps/analysis/alab_release/2021-03-31_release/valhalla/2021-03-31_release/gisaid_metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_meta = pd.read_csv('/Users/al/Documents/scripps/analysis/alab_release/2021-03-31_release/valhalla/2021-03-31_release/metadata.csv')\n",
    "git_meta['collection_date'] = git_meta['collection_date'].astype(str)\n",
    "git_meta.to_csv('/Users/al/Documents/scripps/analysis/alab_release/2021-03-31_release/valhalla/2021-03-31_release/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a': 2}\n",
    "list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from alab_release import *\n",
    "import bjorn_support as bs\n",
    "import mutations as bm\n",
    "# import unsupervised_learning as bul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ids = [f\"SEARCH-{idx}\" for idx in [7338, 7443, 7458, 7600]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.read_csv('/Users/al/Documents/scripps/analysis/bjorn/test_analysis/SARS-CoV-2_sequence_tracker-GISAID.csv')\n",
    "s = s.loc[s['SEARCH SampleID'].isin(s_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.to_csv('/Users/al/Documents/scripps/analysis/bjorn/test_analysis/SARS-CoV-2_sequence_tracker-GISAID.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subs_fp = '/valhalla/dev/replacements.csv'\n",
    "# subs = pd.read_csv(subs_fp)\n",
    "# subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = bs.load_fasta('/valhalla/dev/msa/dev_aligned.fa', is_aligned=True)\n",
    "meta_fp = '/valhalla/dev/metadata.csv'\n",
    "inss = bm.identify_insertions(msa, meta_fp)\n",
    "subs = bm.identify_replacements(msa, meta_fp)\n",
    "dels = bm.identify_deletions(msa, meta_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs['gene'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonconcerning_genes = ['5UTR', 'ORF7a', 'ORF7b', 'ORF8', 'ORF10', 'Non-coding region']\n",
    "subs_flag = ((subs['alt_aa']=='*') & (~subs['gene'].isin(nonconcerning_genes)))\n",
    "sus_subs_ids = subs.loc[subs_flag, 'samples'].str.split(',').explode().unique().tolist()\n",
    "sus_subs_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs.loc[subs_flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dels_flag = ((dels['is_frameshift']==True) & (~dels['gene'].isin(nonconcerning_genes)))\n",
    "sus_dels_ids = dels.loc[dels_flag, 'samples'].str.split(',').explode().unique().tolist()\n",
    "sus_dels_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dels.loc[dels_flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inss[(inss['is_frameshift']==True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inss.loc[ins_flag, 'samples']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_flag = ((inss['is_frameshift']==True) & (~inss['gene'].isin(nonconcerning_genes)))\n",
    "sus_ins_ids = inss.loc[ins_flag, 'samples'].str.split(',').explode().unique().tolist()\n",
    "sus_ins_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus_ids = list(set(sus_subs_ids + sus_dels_ids + sus_ins_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sus_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa = bs.load_fasta('/valhalla/dev/msa/dev_aligned.fa', is_aligned=True)\n",
    "good_seqs = []\n",
    "poor_seqs = []\n",
    "for rec in msa:\n",
    "    if rec.id in sus_ids:\n",
    "        poor_seqs.append(rec)\n",
    "    else:\n",
    "        good_seqs.append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_msa = Align.MultipleSeqAlignment(good_seqs)\n",
    "poor_msa = Align.MultipleSeqAlignment(poor_seqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_msa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlignIO.write(poor_msa, 'test.fa', 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fp = '/valhalla/fastq/2021.03.19/samples.tsv'\n",
    "s_df = pd.read_csv(s_fp, sep='\\t')\n",
    "s_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fp = '/home/al/code/HCoV-19-Genomics/metadata.csv'\n",
    "m = pd.read_csv(m_fp)\n",
    "m.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.sort_values('collection_date', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_fp = '/valhalla/fastq/2021.03.19/Fastq/samples.tsv'\n",
    "samples = pd.read_csv(samples_fp, sep='\\t')\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples['sample'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_fp = '/valhalla/dev/msa/dev.fa'\n",
    "ref_fp = '/home/al/data/hcov19/NC045512.fasta'\n",
    "viral_msa_fp = '/home/al/code/ViralMSA.py'\n",
    "bs.align_fasta_viralMSA(fa_fp, '/valhalla/dev/msa/test_msa', ref_fp=ref_fp, viralmsa_fp=viral_msa_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msa_data = bs.load_fasta('/valhalla/dev/msa/test_msa/dev.fa.aln', is_aligned=True)\n",
    "list(msa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_fp = '/valhalla/2021-03-17_release/replacements.csv'\n",
    "subs = pd.read_csv(subs_fp)\n",
    "subs.loc[subs['alt_aa']=='*']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserts_fp = '/valhalla/dev/insertions.csv'\n",
    "inserts = pd.read_csv(inserts_fp)\n",
    "inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserts_fp = '/valhalla/dev/deletions.csv'\n",
    "inserts = pd.read_csv(inserts_fp)\n",
    "inserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_fp = '/home/al/code/HCoV-19-Genomics/metadata.csv'\n",
    "meta_df = pd.read_csv(meta_fp)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_df.to_csv('/home/al/code/HCoV-19-Genomics/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3827-3575"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tijuana\n",
    "283-275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imperial\n",
    "131-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# riverside\n",
    "28 -23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta_df.loc[meta_df['location']=='MEX/Baja California/Tijuana', 'location'] = 'Mexico/Baja California/Tijuana'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['location'].value_counts().sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_fpath = '/valhalla/analysis'\n",
    "sample_sheet_fpath = '/home/al/analysis/alab_release/SARS-CoV-2_sequence_tracker-GISAID.csv'\n",
    "released_samples_fpath = '/home/al/code/HCoV-19-Genomics/metadata.csv'\n",
    "include_bams = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab all filepaths for bam data\n",
    "bam_filepaths = glob.glob(f\"{analysis_fpath}/**/merged_aligned_bams/illumina/*.bam\")\n",
    "bam_filepaths = [Path(fp) for fp in bam_filepaths]\n",
    "# consolidate sample ID format\n",
    "bam_ids = get_ids(bam_filepaths)\n",
    "# Turn into dataframe\n",
    "bam_data = list(zip(*[bam_ids, bam_filepaths]))\n",
    "bam_df = pd.DataFrame(data=bam_data, columns=['sample_id', 'PATH'])\n",
    "# grab all paths to consensus sequences\n",
    "consensus_filepaths = glob.glob(f\"{analysis_fpath}/**/consensus_sequences/illumina/*.fa\")\n",
    "consensus_filepaths = [Path(fp) for fp in consensus_filepaths]\n",
    "# consolidate sample ID format\n",
    "consensus_ids = get_ids(consensus_filepaths)\n",
    "# Turn into dataframe\n",
    "consensus_data = list(zip(*[consensus_ids, consensus_filepaths]))\n",
    "consensus_df = pd.DataFrame(data=consensus_data, columns=['sample_id', 'PATH'])\n",
    "# clean up cns and bam (remove duplicate IDs)\n",
    "bam_df.drop_duplicates(subset=['sample_id'], keep='last', inplace=True)\n",
    "consensus_df.drop_duplicates(subset=['sample_id'], keep='last', inplace=True)\n",
    "# include only SEARCH samples\n",
    "consensus_df = consensus_df[(consensus_df['sample_id'].str.contains('SEARCH'))]\n",
    "# merge consensus and bam filepaths for each sample ID\n",
    "analysis_df = pd.merge(consensus_df, bam_df, on='sample_id', how='left')\n",
    "# load sample sheet data (GISAID) - make sure to download most recent one\n",
    "seqsum = pd.read_csv(sample_sheet_fpath)\n",
    "# clean up\n",
    "seqsum = seqsum[(~seqsum['SEARCH SampleID'].isna()) & (seqsum['SEARCH SampleID']!='#REF!')]\n",
    "# consolidate sample ID format\n",
    "seqsum.loc[:, 'sample_id'] = seqsum['SEARCH SampleID'].apply(process_id)\n",
    "seqsum.drop_duplicates(subset=['sample_id'], keep='last', inplace=True)\n",
    "seqsum = seqsum[seqsum['New sequences ready for release'] == 'Yes']\n",
    "num_seqs_to_release = seqsum['sample_id'].unique().shape[0]\n",
    "# JOIN summary sheet with analysis meta data\n",
    "sequence_results = pd.merge(seqsum, analysis_df, on='sample_id', how='inner')\n",
    "# compute number of samples with missing consensus and/or bam files\n",
    "num_seqs_found = sequence_results['sample_id'].unique().shape[0]\n",
    "num_samples_missing_cons = num_seqs_to_release - num_seqs_found\n",
    "num_samples_missing_bams = 'NA'\n",
    "if include_bams:\n",
    "    # exclude any samples that do not have BAM data\n",
    "    num_samples_missing_bams = sequence_results[sequence_results['PATH_y'].isna()].shape[0]\n",
    "    sequence_results = sequence_results[~sequence_results['PATH_y'].isna()]\n",
    "# samples missing consensus or BAM sequence files\n",
    "num_samples_missing_bams = sequence_results[sequence_results['PATH_y'].isna()].shape[0]\n",
    "num_samples_missing_cons = sequence_results[sequence_results['PATH_x'].isna()].shape[0]\n",
    "# ## Make sure to remove any samples that have already been uploaded to github (just an extra safety step)\n",
    "# load metadata.csv from github repo, then clean up\n",
    "meta_df = pd.read_csv(released_samples_fpath)\n",
    "meta_df = meta_df[meta_df['ID'].str.contains('SEARCH')]\n",
    "# consolidate sample ID format\n",
    "meta_df.loc[:, 'sample_id'] = meta_df['ID'].apply(process_id)\n",
    "# meta_df['sample_id']\n",
    "# get IDs of samples that have already been released\n",
    "released_seqs = meta_df['sample_id'].unique()\n",
    "# filter out released samples from all the samples we got\n",
    "final_result = sequence_results[~sequence_results['sample_id'].isin(released_seqs)]\n",
    "print(f\"Preparing {final_result.shape[0]} samples for release\")\n",
    "# ## Getting coverage information\n",
    "cov_filepaths = glob.glob(\"{}/**/trimmed_bams/illumina/reports/*.tsv\".format(analysis_fpath))\n",
    "# get_ipython().getoutput(\"find {analysis_fpath} -type f -path '*trimmed_bams/illumina/reports*' -name '*.tsv'\")\n",
    "cov_filepaths = [Path(fp) for fp in cov_filepaths]\n",
    "# read coverage data and clean it up\n",
    "cov_df = pd.concat((pd.read_csv(f, sep='\\t').assign(path=f) for f in cov_filepaths))\n",
    "cov_df.loc[:,'sample_id'] = cov_df['SAMPLE'].apply(process_coverage_sample_ids)\n",
    "cov_df.loc[:,'date'] = cov_df['path'].apply(lambda x: ''.join(x.split('/')[4].split('.')[:3]))\n",
    "cov_df = (cov_df.sort_values('date')\n",
    "          .drop_duplicates(subset=['sample_id'], keep='last'))\n",
    "# JOIN results with coverage info\n",
    "ans = (\n",
    "pd.merge(final_result, cov_df,\n",
    "         on='sample_id', how='left')\n",
    "  .assign(\n",
    "    collection_date = lambda x: pd.to_datetime(x[\"Collection date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    ")\n",
    "  .rename(columns={\n",
    "    \"SEARCH SampleID\": \"ID\",\n",
    "    \"Location\": \"location\",\n",
    "    \"COVERAGE\": \"percent_coverage_cds\",\n",
    "    \"AVG_DEPTH\": \"avg_depth\",\n",
    "    \"Authors\": \"authors\",\n",
    "    \"Originating lab\": \"originating_lab\"\n",
    "})\n",
    ")\n",
    "ans['fasta_hdr'] = ans['Virus name']\n",
    "num_samples_missing_coverage = ans[ans['percent_coverage_cds'].isna()].shape[0]\n",
    "# compute number of samples below 90% coverage\n",
    "low_coverage_samples = ans[ans[\"percent_coverage_cds\"] < 90]\n",
    "# ignore samples below 90% coverage\n",
    "ans = ans[ans[\"percent_coverage_cds\"] >= 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqsum = pd.read_csv(sample_sheet_fpath)\n",
    "seqsum['New sequences ready for release'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/valhalla/fastq/2021.03.12/samples.tsv'\n",
    "samples = pd.read_csv(fp, sep='\\t')\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_data_fp = 'new_api_data.json.gz'\n",
    "gcloud_stat_cmd = f'/home/al/code/google-cloud-sdk/bin/gsutil stat gs://andersen-lab_temp/outbreak_genomics/new_api_data.json.gz'\n",
    "t = bs.run_command_log(gcloud_stat_cmd)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = '/valhalla/gisaid/workflow/mutations_2021-03-09.csv'\n",
    "muts = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muts.loc[muts['country']=='United States', 'location_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "muts = muts[muts['is_synonymous']==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage = 'B.1.526'\n",
    "data = (muts.loc[muts['pangolin_lineage']==lineage, 'mutation']\n",
    "        .value_counts()\n",
    "        .to_frame()\n",
    "        .reset_index()\n",
    "        .rename(columns={'index': 'name', 'mutation': 'count'}))#.values\n",
    "data['frequency'] = (data['count'] - np.min(data['count']))/np.ptp(data['count'])\n",
    "data = data[data['frequency']>=0.05]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.stats import mode\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from skimage.io import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_unsupervised(X:np.ndarray, k:int, centroids=None, tolerance=1e-2):\n",
    "    \"\"\"K-Means Clustering with unsupervised learning trick to infer characteristic mutations\n",
    "    for a given SARS-CoV-2 lineage\"\"\"\n",
    "    # Initialize Input, Centroids, Clusters\n",
    "    if len(X.shape) < 2: X = np.expand_dims(X, axis=1)\n",
    "    if centroids: \n",
    "        centroids = bul.init_centroids(X, k)\n",
    "    else:\n",
    "        centroids = X[np.random.choice(X.shape[0], size=k, replace=False), :]\n",
    "    clusters = [[] for c in centroids]\n",
    "    # Until Convergence\n",
    "    while(True):\n",
    "        # Centroid Norm at t-1\n",
    "        old_centroids_norm = np.linalg.norm(centroids)\n",
    "        # Minimum Distance to Centroid Per Record\n",
    "        js = np.argmin(distance.cdist(X, centroids), axis=1)\n",
    "        # Assign Records to Clusters\n",
    "        for i, c in enumerate(clusters):\n",
    "             clusters[i] = list(np.where(js == i)[0])\n",
    "        # Recompute Cluster Centroids\n",
    "        for j, c in enumerate(centroids):\n",
    "            centroids[j] = np.mean(X[clusters[j]], axis=0)\n",
    "        # Centroid Norm at t\n",
    "        centroids_norm = np.linalg.norm(centroids)\n",
    "        # Convergence!\n",
    "        if np.abs(centroids_norm - old_centroids_norm) < tolerance:\n",
    "            preds = np.zeros_like(X)\n",
    "            clusters = np.asarray(clusters)[np.argsort(centroids.squeeze())[::-1]]\n",
    "#             return clusters, centroids\n",
    "            for i, c in enumerate(clusters):\n",
    "                preds[c] = i\n",
    "            return preds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(data['frequency'].to_numpy(), axis=1)\n",
    "preds = kmeans_unsupervised(X, k=2, centroids=\"kmeans++\") == 0\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['is_characteristic'] = bul.spectral_clustering_unsupervised(X, num_classes=2, n_trees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_characteristic'] = preds\n",
    "data['lineage'] = lineage\n",
    "data.to_csv(f'/Users/al/Documents/scripps/analysis/bjorn/{lineage}_char_mutations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bjorn",
   "language": "python",
   "name": "bjorn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}